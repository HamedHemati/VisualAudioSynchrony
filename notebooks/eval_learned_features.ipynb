{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from vasync.models.pretext_model import ClassifierPretextModel\n",
    "from vasync.utils.common import load_config\n",
    "from PIL import  Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path, checkpoint_path):\n",
    "    config = load_config(config_path)\n",
    "    pretext_base_model = config[\"pretext_base_model\"]\n",
    "    num_cls = config[\"num_cls\"]\n",
    "    model = ClassifierPretextModel(pretext_base_model=pretext_base_model, \n",
    "                                   num_cls=num_cls)\n",
    "    state_dict = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../outputs/img_cls/img_cls_pseudolbl_30_simplemodel/checkpoint_epoch_120.pt\"\n",
    "config_path = \"../experiments/img_cls_pseudolbl_30_simplemodel/config.yml\"\n",
    "model = load_model(config_path, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                                transforms.CenterCrop(128),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"image.jpg\"\n",
    "img = Image.open(image_path)\n",
    "img = transform(img)\n",
    "# img = torch.randn(3, 245, 245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cls = model(img.unsqueeze(0))\n",
    "out_feat = feature_extractor(img.unsqueeze(0))\n",
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7, grad_fn=<NotImplemented>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhemati/anaconda3/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cls = torch.argmax(softmax(out_cls.squeeze(0)))\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6244,  1.0701,  0.0607,  0.8193, -0.4758, -2.1899,  0.6276, -2.1713,\n",
      "         1.0125,  0.1059, -0.3276, -1.2722, -1.2844,  1.7387, -0.7716, -1.3810,\n",
      "         0.1344,  0.3804,  5.0403, -0.5139, -0.6131, -1.1882,  0.8801, -0.2364,\n",
      "         2.7032,  4.1501, -0.0400,  2.0320,  0.5742,  0.4172,  1.1221, -0.0755,\n",
      "        -1.0917,  3.1130,  0.7743,  0.0090,  1.2488,  1.1090,  3.1241, -2.3376,\n",
      "         1.2652, -1.3689,  0.0144,  2.0442,  1.9202, -0.2407,  0.6140, -1.5382,\n",
      "        -0.4002, -1.0114,  5.0189, -0.0656, -0.0445, -1.6074, -0.6707, -0.6546,\n",
      "        -2.7739, -1.1893, -0.3947,  1.5430, -0.8292, -1.4993, -4.3058, -1.9539,\n",
      "         0.5377, -0.4235, -1.5154,  1.3007,  0.4153, -0.0808,  2.2412, -0.2262,\n",
      "        -3.5547, -0.1657,  1.8538, -0.6902,  0.3810,  1.0858, -4.7537,  1.0108,\n",
      "        -0.8477,  3.8216,  0.6973, -1.5183,  1.6745, -0.1210,  1.3584, -1.6053,\n",
      "        -0.4407,  1.1156,  0.8643,  1.1235,  0.3395,  2.3204,  1.6644, -1.5192,\n",
      "         0.5281,  0.2732, -0.4523, -0.7727,  0.0826,  1.5801, -0.6149, -0.1708,\n",
      "        -1.2281,  0.6016, -0.9663,  1.0441,  5.6987,  0.3252, -3.1860,  0.1884,\n",
      "         0.4399, -1.4781,  0.5148,  2.5391,  1.3603, -2.6147,  2.0677,  0.0148,\n",
      "         1.8651,  0.5929,  0.2668,  1.0053,  0.5604, -1.3690,  1.6767,  1.6270],\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor(0.2021, grad_fn=<MeanBackward0>)\n",
      "tensor(1.6890, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_feat.view(-1))\n",
    "print(torch.mean(out_feat))\n",
    "print(torch.std(out_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
